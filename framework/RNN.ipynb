{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b663d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np \n",
    "from activation.tanh import Tanh  \n",
    "from activation.Softmax import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38744ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(n_features, n_hidden, n_output):\n",
    "    # Xavier initialization\n",
    "    Wxh = np.random.randn(n_hidden, n_features) / np.sqrt(n_features)\n",
    "    Whh = np.random.randn(n_hidden, n_hidden) / np.sqrt(n_hidden)\n",
    "    Why = np.random.randn(n_output, n_hidden) / np.sqrt(n_hidden)\n",
    "\n",
    "    bh = np.zeros((n_hidden, 1))\n",
    "    by = np.zeros((n_output, 1))\n",
    "    \n",
    "    return Wxh, Whh, Why, bh, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7dff633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, Wxh, Whh, Why, bh, by):\n",
    "    # many to one RNN\n",
    "    h_prev = np.zeros((Whh.shape[0], 1))  # Initial hidden state\n",
    "    hs = []\n",
    "    for t in range(X.shape[0]):\n",
    "        x_t = X[t].reshape(-1, 1)  # Column vector\n",
    "        h_t = Tanh().forward(np.dot(Wxh, x_t) + np.dot(Whh, h_prev) + bh)\n",
    "        hs.append(h_t)\n",
    "        h_prev = h_t\n",
    "    \n",
    "    y = softmax(np.dot(Why, hs[-1]) + by)\n",
    "    return y, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7033dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(dWxh, dWhh, dWhy, dbh, dby, clip_value=5):\n",
    "    for d in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(d, -clip_value, clip_value, out=d)\n",
    "    return dWxh, dWhh, dWhy, dbh, dby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e1a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y_true, y_pred, hs, Wxh, Whh, Why, bh):\n",
    "    \"\"\"\n",
    "    Backpropagation through time (BPTT)\n",
    "    \n",
    "    Args:\n",
    "        X: input sequence of shape (seq_len, input_size)\n",
    "        y_true: true output (output_size, 1)\n",
    "        y_pred: predicted output (output_size, 1)\n",
    "        hs: list of hidden states, each of shape (hidden_size, 1)\n",
    "        Wxh, Whh, Why: weight matrices\n",
    "        bh: hidden bias (hidden_size, 1)\n",
    "        \n",
    "    Returns:\n",
    "        Gradients: dWxh, dWhh, dWhy, dbh, dby\n",
    "    \"\"\"\n",
    "    seq_len = X.shape[0]\n",
    "    hidden_size = hs[0].shape[0]\n",
    "\n",
    "    # Initialize gradients\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWhy = np.dot((y_pred - y_true), hs[-1].T)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dby = y_pred - y_true\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(seq_len)):\n",
    "        dh = np.dot(Why.T, dby) + dh_next\n",
    "        dh_raw = Tanh.derivative(hs[t]) * dh  \n",
    "\n",
    "        dbh += dh_raw\n",
    "        dWxh += np.dot(dh_raw, X[t].reshape(1, -1))  # (hidden, input)\n",
    "        if t > 0:\n",
    "            dWhh += np.dot(dh_raw, hs[t-1].T)\n",
    "\n",
    "        # Update dh_next for next timestep (propagate backward)\n",
    "        dh_next = np.dot(Whh.T, dh_raw)\n",
    "        \n",
    "    # Clip gradients to avoid exploding gradients    \n",
    "    dWxh, dWhh, dWhy, dbh, dby = clip_gradients(dWxh, dWhh, dWhy, dbh, dby, clip_value=5)\n",
    "    return dWxh, dWhh, dWhy, dbh, dby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "421dd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(Wxh, Whh, Why, bh, by, dWxh, dWhh, dWhy, dbh, dby, eta, clip_value=5):\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    for d in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(d, -clip_value, clip_value, out=d)\n",
    "\n",
    "    Wxh -= eta * dWxh\n",
    "    Whh -= eta * dWhh\n",
    "    Why -= eta * dWhy\n",
    "    bh  -= eta * dbh\n",
    "    by  -= eta * dby\n",
    "    return Wxh, Whh, Why, bh, by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "836ec738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y_true, Wxh, Whh, Why, bh, by, eta):\n",
    "    y_pred, hs = forward(X, Wxh, Whh, Why, bh, by)\n",
    "    dWxh, dWhh, dWhy, dbh, dby = backward(X, y_true, y_pred, hs, Wxh, Whh, Why, bh)\n",
    "    Wxh, Whh, Why, bh, by = update_params(Wxh, Whh, Why, bh, by, dWxh, dWhh, dWhy, dbh, dby, eta)\n",
    "    return Wxh, Whh, Why, bh, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bebf1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Wxh, Whh, Why, bh, by):\n",
    "    y_pred, _ = forward(X, Wxh, Whh, Why, bh, by)\n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
